# 模型

## 新增字段

- 文件：`src\platform\endpoint\common\endpointProvider.ts`
- 修改接口: `IModelAPIResponse`
- 修改后

    ```ts
    export interface IModelAPIResponse {
        id: string;
        name: string;
        policy?: ModelPolicy;
        model_picker_enabled: boolean;
        preview?: boolean;
        baseUrl?: string;				// 新增字段
        apiKey?: string;				// 新增字段
        model?: string;					// 新增字段
        is_chat_default: boolean;
        is_chat_fallback: boolean;
        version: string;
        warning_messages?: { code: string; message: string }[];
        info_messages?: { code: string; message: string }[];
        billing?: { is_premium: boolean; multiplier: number; restricted_to?: string[] };
        capabilities: IChatModelCapabilities | ICompletionModelCapabilities | IEmbeddingModelCapabilities;
        supported_endpoints?: ModelSupportedEndpoint[];
        custom_model?: { key_name: string; owner_name: string };
    }

    ```

- 文件：`node_modules\@vscode\copilot-api\dist\types.d.ts`
- 修改接口：`RequestMetadata`
- 修改：在原有数据结构上追加新字段

    ```ts
    export type RequestMetadata = {
        baseUrl?: string;
        apiKey?: string;
        model?: string;
    } & (
        {
            /* .... */
        }
    )
    ```

## urlOrRequestMetadata 修改

- 文件： `src\platform\endpoint\node\chatEndpoint.ts`
- 类：`ChatEndpoint`
- 修改实现

```ts
public get urlOrRequestMetadata(): string | RequestMetadata {
    // Use override or respect setting.
    // TODO unlikely but would break if it changes in the middle of a request being constructed
    return this.modelMetadata.urlOrRequestMetadata ??
        (this.useResponsesApi ? { type: RequestType.ChatResponses, baseUrl: this.modelMetadata.baseUrl, apiKey: this.modelMetadata.apiKey } :
            this.useMessagesApi ? { type: RequestType.ChatMessages, baseUrl: this.modelMetadata.baseUrl, apiKey: this.modelMetadata.apiKey, model: this.modelMetadata.model } : { type: RequestType.ChatCompletions, baseUrl: this.modelMetadata.baseUrl, apiKey: this.modelMetadata.apiKey });
}
```

- 文件 `src\platform\endpoint\node\embeddingsEndpoint.ts`
- 类：`EmbeddingEndpoint`
- 修改函数实现

```ts
public get urlOrRequestMetadata(): string | RequestMetadata {
    return { type: RequestType.CAPIEmbeddings, modelId: LEGACY_EMBEDDING_MODEL_ID.TEXT3SMALL, baseUrl: this._modelInfo.baseUrl, apiKey: this._modelInfo.apiKey, model: this._modelInfo.model };
}
```

- 文件 `src\extension\agents\claude\node\claudeLanguageModelServer.ts`
- 类 `ClaudeStreamingPassThroughEndpoint`
- 修改函数实现

```ts
public get urlOrRequestMetadata(): string | RequestMetadata {
    // Force Messages API endpoint - we need this regardless of the useMessagesApi setting
    // since we're proxying Messages API format requests from Claude Code
    const baseUrl = this.base.urlOrRequestMetadata;
    if (typeof baseUrl === 'string') {
        return baseUrl;
    }
    return { type: RequestType.ChatMessages, baseUrl: baseUrl.baseUrl, apiKey: baseUrl.apiKey, model: baseUrl.model };
}

```

## 模型获取


- 文件： `src\platform\endpoint\node\modelMetadataFetcher.ts`
- 类： `ModelMetadataFetcher`
- 修改函数实现

```ts
private async _fetchModels(force?: boolean): Promise<void> {
    if (!force && !this._shouldRefreshModels()) {
        return;
    }

    const requestId = generateUuid();
    const requestMetadata = { type: RequestType.Models, isModelLab: this._isModelLab };

    try {
        const base_config = workspace.getConfiguration('github.copilot.hackModels.base');
        const inline_config = workspace.getConfiguration('github.copilot.hackModels.inline');
        const embedding_config = workspace.getConfiguration('github.copilot.hackModels.embedding');
        const fast_config = workspace.getConfiguration('github.copilot.hackModels.fast');
        const claude_config = workspace.getConfiguration('github.copilot.hackModels.claude');

        const models: IModelAPIResponse[] = [
            // base
            {
                id: "gpt-5-mini",
                preview: false,
                baseUrl: base_config.get("baseUrl", "http://steam"),
                apiKey: base_config.get("apiKey", "xxxxx"),
                name: base_config.get("model", "gpt-5.2"),
                model: base_config.get("model", "gpt-5.2"),
                is_chat_default: base_config.get("is_chat_default", true),
                is_chat_fallback: true,
                model_picker_enabled: base_config.get("model_picker_enabled", true),
                version: base_config.get("version", "v1.0.0"),
                supported_endpoints: base_config.get("supported_endpoints", [ModelSupportedEndpoint.ChatCompletions]) as ModelSupportedEndpoint[],
                capabilities: {
                    type: "chat",
                    family: "gpt-5-mini",
                    tokenizer: base_config.get("capabilities.tokenizer", TokenizerType.O200K),
                    limits: {
                        max_context_window_tokens: base_config.get("capabilities.limits.max_context_window_tokens", 128000),
                        max_output_tokens: base_config.get("capabilities.limits.max_output_tokens", Math.floor(base_config.get("capabilities.limits.max_context_window_tokens", 128000) * 0.25)),
                        max_prompt_tokens: base_config.get("capabilities.limits.max_prompt_tokens", Math.floor(base_config.get("capabilities.limits.max_context_window_tokens", 128000) * 0.5)),
                        vision: {
                            max_prompt_images: base_config.get("capabilities.limits.vision.max_prompt_images", 1)
                        }
                    },
                    supports: {
                        parallel_tool_calls: base_config.get("capabilities.supports.parallel_tool_calls", true),
                        streaming: base_config.get("capabilities.supports.streaming", true),
                        tool_calls: base_config.get("capabilities.supports.tool_calls", true),
                        vision: base_config.get("capabilities.supports.vision", true)
                    },
                },
                policy: {
                    state: "enabled",
                    terms: "Enable access to the latest GPT-5 mini model from OpenAI. [Learn more about how GitHub Copilot serves GPT-5 mini](https://gh.io/copilot-openai)."
                },
                billing: {
                    is_premium: true,
                    multiplier: 1
                },
            },
            // ChatCompletions
            {
                id: "gpt-41-copilot",
                preview: false,
                baseUrl: inline_config.get("baseUrl", "http://steam"),
                apiKey: inline_config.get("apiKey", "xxxxx"),
                model: inline_config.get("model", "gpt-4.1"),
                name: inline_config.get("model", "gpt-4.1"),
                is_chat_default: inline_config.get("is_chat_default", false),
                is_chat_fallback: inline_config.get("is_chat_fallback", false),
                model_picker_enabled: inline_config.get("model_picker_enabled", true),
                version: inline_config.get("version", "v1.0.0"),
                supported_endpoints: inline_config.get("supported_endpoints", [ModelSupportedEndpoint.ChatCompletions]) as ModelSupportedEndpoint[],
                capabilities: {
                    type: "completion",
                    family: "gpt-4.1",
                    tokenizer: inline_config.get("capabilities.tokenizer", TokenizerType.O200K),
                },
                billing: {
                    is_premium: true,
                    multiplier: 1
                },
            },
            // embedding
            {
                id: "text-embedding-3-small",
                preview: false,
                model_picker_enabled: false,
                baseUrl: embedding_config.get("baseUrl", "http://steam"),
                apiKey: embedding_config.get("apiKey", "xxxxx"),
                model: embedding_config.get("model", "text-embedding-3-small"),
                name: embedding_config.get("model", "text-embedding-3-small"),
                is_chat_default: false,
                is_chat_fallback: false,
                version: embedding_config.get("version", "v1.0.0"),
                capabilities: {
                    type: "embeddings",
                    family: "text-embedding-3-small",
                    chunk_strategy: embedding_config.get("capabilities.chunk_strategy", "token"),
                    tokenizer: embedding_config.get("capabilities.tokenizer", TokenizerType.O200K),
                    limits: {
                        max_inputs: embedding_config.get("capabilities.limits.max_inputs", 10),
                        max_token: embedding_config.get("capabilities.limits.max_token", 250)
                    },
                },
                billing: {
                    is_premium: true,
                    multiplier: 1
                },
            },
            // fast
            {
                id: "gpt-4o-mini",
                preview: false,
                baseUrl: fast_config.get("baseUrl", "http://steam"),
                apiKey: fast_config.get("apiKey", "xxxxx"),
                name: fast_config.get("model", "gpt-4o-mini"),
                model: fast_config.get("model", "gpt-4o-mini"),
                is_chat_default: fast_config.get("is_chat_default", true),
                is_chat_fallback: fast_config.get("is_chat_fallback", false),
                model_picker_enabled: fast_config.get("model_picker_enabled", false),
                version: fast_config.get("version", "v1.0.0"),
                supported_endpoints: fast_config.get("supported_endpoints", [ModelSupportedEndpoint.ChatCompletions]) as ModelSupportedEndpoint[],
                capabilities: {
                    type: "chat",
                    family: "gpt-4o-mini",
                    tokenizer: fast_config.get("capabilities.tokenizer", TokenizerType.O200K),
                    limits: {
                        max_context_window_tokens: fast_config.get("capabilities.limits.max_context_window_tokens", 128000),
                        max_output_tokens: fast_config.get("capabilities.limits.max_output_tokens", Math.floor(base_config.get("capabilities.limits.max_context_window_tokens", 128000) * 0.25)),
                        max_prompt_tokens: fast_config.get("capabilities.limits.max_prompt_tokens", Math.floor(base_config.get("capabilities.limits.max_context_window_tokens", 128000) * 0.5)),
                        vision: {
                            max_prompt_images: fast_config.get("capabilities.limits.vision.max_prompt_images", 1)
                        }
                    },
                    supports: {
                        parallel_tool_calls: fast_config.get("capabilities.supports.parallel_tool_calls", true),
                        streaming: fast_config.get("capabilities.supports.streaming", true),
                        tool_calls: fast_config.get("capabilities.supports.tool_calls", true),
                        vision: fast_config.get("capabilities.supports.vision", true)
                    },
                },
                billing: {
                    is_premium: true,
                    multiplier: 1
                },
            },
            {
                id: "gpt-4.1",
                preview: false,
                baseUrl: fast_config.get("baseUrl", "http://steam"),
                apiKey: fast_config.get("apiKey", "xxxxx"),
                name: fast_config.get("model", "gpt-4.1"),
                model: fast_config.get("model", "gpt-4.1"),
                is_chat_default: fast_config.get("is_chat_default", true),
                is_chat_fallback: fast_config.get("is_chat_fallback", false),
                model_picker_enabled: false,
                version: fast_config.get("version", "v1.0.0"),
                supported_endpoints: fast_config.get("supported_endpoints", [ModelSupportedEndpoint.ChatCompletions]) as ModelSupportedEndpoint[],
                capabilities: {
                    type: "chat",
                    family: "gpt-4.1",
                    tokenizer: fast_config.get("capabilities.tokenizer", TokenizerType.O200K),
                    limits: {
                        max_context_window_tokens: fast_config.get("capabilities.limits.max_context_window_tokens", 128000),
                        max_output_tokens: fast_config.get("capabilities.limits.max_output_tokens", Math.floor(base_config.get("capabilities.limits.max_context_window_tokens", 128000) * 0.25)),
                        max_prompt_tokens: fast_config.get("capabilities.limits.max_prompt_tokens", Math.floor(base_config.get("capabilities.limits.max_context_window_tokens", 128000) * 0.5)),
                        vision: {
                            max_prompt_images: fast_config.get("capabilities.limits.vision.max_prompt_images", 1)
                        }
                    },
                    supports: {
                        parallel_tool_calls: fast_config.get("capabilities.supports.parallel_tool_calls", true),
                        streaming: fast_config.get("capabilities.supports.streaming", true),
                        tool_calls: fast_config.get("capabilities.supports.tool_calls", true),
                        vision: fast_config.get("capabilities.supports.vision", true)
                    },
                },
                billing: {
                    is_premium: true,
                    multiplier: 1
                },
            }
        ];

        const extras = workspace.getConfiguration('github.copilot.hackModels').get('extras', []) as IModelAPIResponse[];
        for (var i = 0; i < extras.length; i++) {
            const item = extras[i];
            try {
                item.id = item.model ?? `custom_${i}`;
                item.name = item.name ?? item.model ?? `Custom Model ${i}`;
                item.model_picker_enabled = true;

                if (item.capabilities.family.length <= 0) {
                    item.capabilities.family = "custom";
                }

                models.push(item);
            } catch (e) {
                this._logService.error(e, `Failed to append extras model ${String(item.model)}.`);
            }
        }

        this._familyMap.clear();

        this._requestLogger.logModelListCall(requestId, requestMetadata, models);
        for (let model of models) {
            model = await this._hydrateResolvedModel(model);
            const isCompletionModel = isCompletionModelInformation(model);
            // The base model is whatever model is deemed "fallback" by the server
            if (model.is_chat_fallback && !isCompletionModel) {
                this._copilotBaseModel = model;
            }
            const family = model.capabilities.family;
            const familyMap = isCompletionModel ? this._completionsFamilyMap : this._familyMap;
            if (!familyMap.has(family)) {
                familyMap.set(family, []);
            }
            familyMap.get(family)?.push(model);
        }

        this._lastFetchTime = Date.now();
        this._lastFetchError = undefined;
        this._onDidModelRefresh.fire();

    } catch (e) {
        this._logService.error(e, `Failed to fetch models (${requestId})`);
        this._lastFetchError = e;
        this._lastFetchTime = 0;
    }
}
```

## 模型刷新

### 模型管理界面

1. 在 `src\platform\endpoint\node\modelMetadataFetcher.ts` 的 `ModelMetadataFetcher` 的构造函数中添加

```ts
this._register(workspace.onDidChangeConfiguration(async e => {
    if (e.affectsConfiguration('github.copilot.hackModels')) {
        await this._fetchModels(true);
    }
}));
```

2. 在 `src\platform\endpoint\common\endpointProvider.ts` 中的 `IEndpointProvider` 添加字段

```ts
	readonly onDidModelsRefresh?: Event<void>;
```

3. `src\extension\prompt\vscode-node\endpointProviderImpl.ts` 中 `ProductionEndpointProvider` 实现 `onDidModelsRefresh`

```ts
    private readonly _onDidModelsRefresh = new Emitter<void>();

    get onDidModelsRefresh(): Event<void> {
        return this._onDidModelsRefresh.event;
    }
```

4. 在 `src\extension\prompt\vscode-node\endpointProviderImpl.ts` 中 `ProductionEndpointProvider` 构造器里注册的事件

```ts
    // When new models come in from CAPI we want to clear our local caches and let the endpoints be recreated since there may be new info
    this._modelFetcher.onDidModelsRefresh(() => {
        this._chatEndpoints.clear();
        // Notify Language Model UI to refresh the model list
        this._onDidModelsRefresh.fire();
    });
```

5. 在 `src\extension\conversation\vscode-node\languageModelAccess.ts` 中 `LanguageModelAccess` 构造器里注册事件

```ts
// Listen to endpoint provider model refresh events to update the Language Models UI
if (this._endpointProvider.onDidModelsRefresh) {
    this._register(this._endpointProvider.onDidModelsRefresh(() => {
        this._logService.debug('[LanguageModelAccess] Models refreshed, updating Language Models UI');
        this._onDidChange.fire();
    }));
}
```

最终实现 language Models 界面刷新

 ### claude 模型

- 文件 `src\extension\agents\claude\node\claudeCodeModels.ts`
- 类 `ClaudeCodeModels`
- 修改函数实现

```ts
public async getModels(): Promise<ClaudeCodeModelInfo[]> {
    return this._getAvailableModels();
}
```

- 文件 `src\extension\chatSessions\vscode-node\claudeChatSessionContentProvider.ts`
- 类 `ClaudeChatSessionContentProvider`
- 修改构造函数实现

```ts
constructor(
    @IEndpointProvider private readonly endpointProvider: IEndpointProvider,
    @IClaudeCodeSessionService private readonly sessionService: IClaudeCodeSessionService,
    @IClaudeCodeModels private readonly claudeCodeModels: IClaudeCodeModels,
    @IClaudeSessionStateService private readonly sessionStateService: IClaudeSessionStateService,
    @IConfigurationService private readonly configurationService: IConfigurationService,
) {
    super();

    // 重新加载模型
    if (this.endpointProvider.onDidModelsRefresh) {
        this.endpointProvider.onDidModelsRefresh(() => {
            this._onDidChangeChatSessionProviderOptions.fire();
        });
    }

    // Listen for configuration changes to update available options
    this._register(this.configurationService.onDidChangeConfiguration(e => {
        if (e.affectsConfiguration(ConfigKey.ClaudeAgentAllowDangerouslySkipPermissions.fullyQualifiedId)) {
            this._onDidChangeChatSessionProviderOptions.fire();
        }
    }));

    // Listen for state changes and notify UI only if value actually changed
    this._register(this.sessionStateService.onDidChangeSessionState(e => {
        const lastKnown = this._lastKnownOptions.get(e.sessionId);
        const updates: { optionId: string; value: string }[] = [];

        if (e.modelId !== undefined && e.modelId !== lastKnown?.modelId) {
            updates.push({ optionId: MODELS_OPTION_ID, value: e.modelId });
            this._updateLastKnown(e.sessionId, { modelId: e.modelId });
        }
        if (e.permissionMode !== undefined && e.permissionMode !== lastKnown?.permissionMode) {
            updates.push({ optionId: PERMISSION_MODE_OPTION_ID, value: e.permissionMode });
            this._updateLastKnown(e.sessionId, { permissionMode: e.permissionMode });
        }

        if (updates.length > 0) {
            const resource = ClaudeSessionUri.forSessionId(e.sessionId);
            this._onDidChangeChatSessionOptions.fire({ resource, updates });
        }
    }));
}
```

## autoMode

- 文件：`src\platform\endpoint\node\automodeService.ts`
- 类: `AutoModeTokenBank`
- 修改函数实现

```ts
private async _fetchToken(): Promise<void> {

    const data: AutoModeAPIResponse = {
        available_models: ["gpt-5-mini", "gpt-4.1", "gpt-4o"],
        selected_model: "gpt-5-mini",
        session_token: "eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdmFpbGFibGVfbW9kZWxzIjpbImdwdC01LW1pbmkiLCJncHQtNC4xIiwiZ3B0LTRvIl0sInNlbGVjdGVkX21vZGVsIjoiZ3B0LTUtbWluaSIsInN1YiI6IjllNTdmMGJmMjY5YzNjZWQ2NTZkZTgzMjQzMGZlNDc3IiwiaWF0IjoxNzcwNDI5MjQ2LCJleHAiOjE3NzA0MzI4NDZ9.V1sHTOUW0xGgARlOC1gT-QxqOx2Pl2qGMAfC8BhoFYC_OamHreu0xsW2LcO29TRRSLdxWhG9r502uzIzGjhQfg",
        expires_at: 2770432846
    };

    data.expires_at = Math.floor(Date.now() / 1000) + 600;

    this._token = data;
    // Trigger a refresh 5 minutes before expiration
    if (!this._store.isDisposed) {
        this._refreshTimer.cancelAndSet(this._fetchToken.bind(this), (data.expires_at * 1000) - Date.now() - 5 * 60 * 1000);
    }
    this._fetchTokenPromise = undefined;
}
```

